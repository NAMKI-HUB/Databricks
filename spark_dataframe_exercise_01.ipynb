{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f87cac-0a22-4a0c-a24e-e83757063089",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### titanic_train.csv 파일을 로드하고, 이를 DataFrame으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c410ad4-7961-4a16-901f-3a5a44c50ebf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d39034-1d83-4d88-b9d7-d29ef5e3e8dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.read.csv() 메소드를 이용하여 csv 파일을 로드하고 DataFrame으로 변환. \n",
    "# pandas_df = pd.read_csv('/FileStore/tables/titanic_train.csv', header='infer')\n",
    "titanic_sdf = spark.read.csv('/FileStore/train.csv', header=True, inferSchema=True)\n",
    "print('titanic sdf type:', type(titanic_sdf))\n",
    "# dbfs:/FileStore/train.csv\n",
    "# Databricks에서는 DataFrame의 컬럼과 데이터를 formatting된 형태로 보기 위해서 display() 지원.  \n",
    "display(titanic_sdf) #z.show(titanic_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e794dfc5-86ad-4eff-b1cf-58d913053289",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# /FileStore는 DBFS 파일 시스템으로 Spark외에는 접근 불가하여 아래는 오류 발생. \n",
    "pandas_df = pd.read_csv('/FileStore/tables/titanic_train.csv', header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "768bcf4f-6a7f-442b-b712-48a8b7fc536f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pandas DataFrame을 spark DataFrame으로 부터 생성. \n",
    "titanic_pdf = titanic_sdf.select('*').toPandas()\n",
    "print(type(titanic_pdf))\n",
    "\n",
    "# display()는 pandas DataFrame에도 적용됨. \n",
    "display(titanic_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7536d43d-7429-4b92-a396-1828eb493e4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark DataFrame을 메모리에 cache\n",
    "titianic_sdf = titanic_sdf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bef7733-f49c-49d1-87fa-517ff574ccd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### pandas DataFrame의 head()와 spark DataFrame head() 비교 및 print() 적용 시 차이\n",
    "* pandas DataFrame의 head(N)는 Dataframe의 선두 N개 Record를 가지는 DataFrame을 반환\n",
    "* spark DataFrame의 head(N)는 DataFrame의 선두 N개 Row Object를 list로 반환. \n",
    "* spark DataFrame의 limit(N)가 DataFrame의 선두 N개 Record를 가지는 DataFrame을 반환. \n",
    "* pandas DataFrame은 print() 적용 시 DataFrame의 내용을 출력하지만 spark DataFrame은 DataFrame의 schema만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c60145c-a9b1-44ce-89a4-f87e4c92c74d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pandas DataFrame.head(N)는 선두 N개 Record를 가지는 DataFrame 반환\n",
    "print(type(titanic_pdf.head(10)))\n",
    "# Pandas DataFrame은 print()로 DataFrame의 내용을 출력. \n",
    "print(titanic_pdf.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60ecc4dc-d7d0-4d52-a6e1-c2a44f253795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark DataFrame의 head(N)는 DataFrame의 선두 N개 Row Object를 list로 반환.\n",
    "print(type(titanic_sdf.head(10)))\n",
    "print(titanic_sdf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d87f1899-0882-4606-a51f-0b69d229aca4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9d3e4c0-b93a-40be-9bd8-dad76ad10074",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark DataFrame의 limit(N)은 DataFrame의 선두 N개 Record를 가지는 DataFrame을 반환.\n",
    "print(type(titanic_sdf.limit(10)))\n",
    "\n",
    "# spark DataFrame은 pandas와 다르게 print() 호출 시 값을 출력하지 않고 DataFrame의 schema만 출력.  \n",
    "print(titanic_sdf.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f612dc4-b8c6-4039-bd14-8ed5b99411ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark DataFrame의 내용을 출력하기 위해서는 show() 메소드를 사용해야 함. \n",
    "print(titanic_sdf.limit(10).show())\n",
    "titanic_sdf.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1843306c-a8b6-42e0-a897-d3e74779199a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks의 display()는 spark DataFrame의 내용을 출력할 수 있음. \n",
    "display(titanic_sdf)\n",
    "\n",
    "# 메모리 절약을 위해 limit() 사용이 바람직 \n",
    "display(titanic_sdf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b2e6366-38b9-4fa6-819f-2293aa11c836",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### pandas DataFrame의 info()에 대응하는 spark DataFrame 메소드와 로직.\n",
    "* pandas DataFrame의 info()는 컬럼명, data type과 not null 건수를 함께 출력\n",
    "* spark DataFrame의 info() 메소드가 없으며 대신 printSchema() 메소드로 스키마(컬럼명, data type)만 출력\n",
    "* not null 건수를 위해서는 별도의 SQL성 쿼리 작성 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f35657ae-49d2-43b8-807a-a720e57a3bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_pdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b531ee7-54d8-414c-8565-792a40cecdc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data type은 java object type\n",
    "titanic_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1063857-3009-47b2-a553-9c6dd27878bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL의 count case when과 유사한 로직으로 null 값을 포함한 컬럼 확인하기 \n",
    "from pyspark.sql.functions import count, isnan, when, col\n",
    "\n",
    "titanic_sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in titanic_sdf.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a6c20c-7f74-4bd8-abf7-9cd802fcfd8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### pandas DataFrame describe()와 spark DataFrame describe() 비교\n",
    "* spark dataframe은 pandas dataframe과 비슷하게 describe()를 통해 모든 컬럼값들의 건수/평균/표준편차/최소값/최대값 등의 정보를 확인할 수 있음. 다만 percentile값을 만들지 않음. \n",
    "* pandas dataframe과 다르게 describe()시 숫자형 컬럼 뿐만 아니라 문자형 컬럼에 대해서도 건수/평균/표준편차/최소값/최대값 조사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4719f227-7aca-49b1-9355-ac90215da034",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_pdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75bb4193-12cd-4d8d-b09a-7fc5d57cbd88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark DataFrame의 describe() 역시 DataFrame을 반환하므로 내용 출력을 위해서는 show() 메소드를 호출하거나 Databricks의 display()로 감싸야 함. \n",
    "display(titanic_sdf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db775dd7-1cdd-47dc-9e0e-aa0fb2d3aa17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24899d7-7385-4913-92a1-e0b3e2ab4a2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# number형 컬럼들에 대해서만 describe()수행할 수 있도록 select 컬럼 filtering 적용. \n",
    "number_columns = [column_name for column_name, dtype in titanic_sdf.dtypes if dtype != 'string']\n",
    "print(number_columns)\n",
    "\n",
    "titanic_sdf.select(number_columns).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08fd1432-95e8-4687-a090-122b0de69485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### pandas DataFrame의 shape에 이에 대응하는 spark Dataframe 로직\n",
    "* pandas DataFrame의 shape는 row건수와 column 개수를 매우 빠르게 제공.\n",
    "* spark DataFrame은 shape를 제공하지 않음. column 개수는 spark DataFrame의 columns로 확인. row건수는 count() 메소드(쿼리성)로 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6399841b-aacc-4902-b88a-2714d42c912e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titanic_pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5bde027-7158-472d-bec2-87e5bfb7a89c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark DataFrame은 columns 속성으로 컬럼명을 list로 반환. \n",
    "print('column들:', titanic_sdf.columns)\n",
    "print('column개수:', len(titanic_sdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1998618d-fe81-4879-9929-29adcff200a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark DataFrame은 count() 메소드로 전체 row 건수를 제공. \n",
    "print(titanic_sdf.count())\n",
    "print(type(titanic_sdf.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58e8c51f-1222-4d9a-bca7-441a677529bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('titanic_sdf shape:', (titanic_sdf.count(), len(titanic_sdf.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6488c09f-27bf-4b28-b656-9de59443921d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark DataFrame의 select() 메소드 알아보기\n",
    "* select() 메소드는 SQL의 Select 절과 유사하게 한개 이상의 컬럼들의 값을 DataFrame형태로 반환. \n",
    "* 한개의 컬럼명, 또는 여러개의 컬럼명을 인자로 입력할 수 있음.\n",
    "* 개별 컬럼명을 문자열 형태로 또는 DataFrame의 컬럼 속성으로 지정\n",
    "* DataFrame의 컬럼 속성으로 지정시에는 DataFrame.컬럼명, DataFrame[컬럼명], col(컬럼명) 으로 지정 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeb95de6-3143-430d-b355-b0e387f3a202",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict_01 = {'Name': ['Chulmin', 'Wansoo','Myunghyun','Hyunjoo', 'Chulman'],\n",
    "           'Year': [2011, 2016, 2015, 2015, 2011],\n",
    "           'Gender': ['Male', 'Male', 'Male', 'Female', 'Male']\n",
    "          }\n",
    "# 딕셔너리를 DataFrame으로 변환\n",
    "data_pdf = pd.DataFrame(dict_01)\n",
    "\n",
    "# pandas DataFrame은 spark DataFrame으로 변환\n",
    "data_sdf = spark.createDataFrame(data_pdf)\n",
    "\n",
    "print(type(data_sdf))\n",
    "display(data_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb0eeb09-2a55-4f34-9a1f-0fce2ac4908f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('pandas DataFrame의 단일 컬럼값을 출력')\n",
    "print(data_pdf['Name'])\n",
    "\n",
    "print('\\npandas DataFrame의 여러개 컬럼값들을 출력')\n",
    "print(data_pdf[['Name', 'Year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2123f9c0-7513-4f38-b8f5-5bb7a71a971f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select() 메소드는 DataFrame을 반환. Name 컬럼을 가지는 DataFrame을 반환. \n",
    "data_sdf.select('Name').show() # select name from data_sdf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32679bac-f0a3-485d-947a-e33adcf220e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 여러개의 컬럼명을 select 시에도 개별 컬럼명을 인자로 넣어줌. pandas와 다르게 list를 사용하지 않음. \n",
    "data_sdf.select('Name', 'Year').show() # select Name, Year from data_sdf\n",
    "\n",
    "# 모든 컬럼을 선택시 SQL과 유사하게 '*' 사용 가능. \n",
    "data_sdf.select('*').show() # select * from data_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5fd004-97aa-4766-8f4d-2b4ee227e7e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select_columns = data_sdf.columns\n",
    "# * list 는 list내부의 원소를 unpack함. \n",
    "print('select_columns:', select_columns, *select_columns)\n",
    "\n",
    "data_sdf.select(*select_columns).show()\n",
    "#버전 upgrade가 되면서 select 절에 list를 입력해도 여러개의 컬럼을 반환 가능. 하지만 원칙적으로 개별 컬럼들을 인자로 입력하는 것이 바람직. \n",
    "data_sdf.select(select_columns).show()\n",
    "\n",
    "select_columns.remove('Name')\n",
    "data_sdf.select(*select_columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf7e4bb9-5281-4e3b-bcb8-372743d1bd26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 컬럼 속성으로 지정하여 select 의 인자로 사용 가능. pandas DataFrame의 ['column']는 컬럼값을 포함한 array를 지칭하나 spark DataFrame의 ['column']는 컬럼 자체를 지정함. \n",
    "data_sdf.select(data_sdf.Name, data_sdf.Year).show()\n",
    "data_sdf.select(data_sdf['Name'], data_sdf['Year']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d933a6e-775d-43c7-bf43-29843bd949f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_pdf.Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65637f3e-377e-4769-b75e-362dbb6b3b39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# pyspark.sql.functions의 col() 함수를 이용하여 명시적으로 컬럼명을 지정할 수 있음. \n",
    "data_sdf.select(col('Name'), col('Year')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81db3d87-bb43-41a5-8ce3-fbae8d4f1cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(data_sdf.Name), type(data_sdf['Name']))\n",
    "print(type(col('Name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b31e42d3-42fb-4755-a7c7-a1b7e1c6e0b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, lower, col\n",
    "\n",
    "# select()에서 컬럼 데이터 가공 후 생성 가능. \n",
    "data_sdf.select(\"*\", upper(col('Name'))).show() # Select A.*, upper(Name) from data_view A\n",
    "data_sdf.select(upper(col('Name')).alias('Cap_Name')).show() # Select upper(Name) as Cap_Name from data_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5368f970-6aa4-4faa-861e-4739614ee5ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### spark DataFrame filter() 메소드 알아보기\n",
    "* filter()는 SQL의 where와 유사하게 DataFrame내의 특정 조건을 만족하는 레코드를 DataFrame으로 반환. \n",
    "* filter()내의 조건 컬럼은 컬럼 속성으로 지정 가능. 조건문 자체는 SQL 과 유사한 문자열로 지정 할 수 있음(조건 컬럼은 문자열 지정이 안됨. )\n",
    "* where() 메소드는 filter()의 alias이며 SQL where와 직관적인 동일성을 간주하기 위해 생성. \n",
    "* 복합 조건 and는 & 를, or를 | 를 사용. 개별 조건은 ()로 감싸야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b571db77-89cf-42c2-9ce5-037c46e387ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict_01 = {'Name': ['Chulmin', 'Wansoo','Myunghyun','Hyunjoo', 'Chulman'],\n",
    "           'Year': [2011, 2016, 2015, 2015, 2011],\n",
    "           'Gender': ['Male', 'Male', 'Male', 'Female', 'Male']\n",
    "          }\n",
    "# 딕셔너리를 DataFrame으로 변환\n",
    "data_pdf = pd.DataFrame(dict_01)\n",
    "\n",
    "# pandas DataFrame은 spark DataFrame으로 변환\n",
    "data_sdf = spark.createDataFrame(data_pdf)\n",
    "\n",
    "print(type(data_sdf))\n",
    "display(data_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4428bdd6-84cf-449e-82d5-8265a6edb267",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_sdf.filter(data_sdf['Name'] == 'Chulmin').show() # select * from data_sdf where Name = 'Chulmin'\n",
    "\n",
    "print('조건 컬럼을 문자열로 지정할 수 없으므로 아래는 오류를 발생 시킵니다. ')\n",
    "data_sdf.filter('Name' == 'Chulmin').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66d892d-6a96-43ee-bb66-8ea99000d839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_sdf.where(data_sdf['Name'] == 'Chulmin').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "944c6562-8f2b-41b3-a7fb-16c8495a4630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 단일 컬럼은 string으로 사용할 수 없으며 filter 절의 조건 자체를 SQL의 where 조건절 구문과 유사하게 string으로 만들어야함. 단 SQL과 다르게 동일 비교 시 = 가 아니라 == 이 사용되어야 함.\n",
    "# 또한 SQL 내부 문자열은 '' 으로 표시해야함. \n",
    "data_sdf.filter(\"Name == 'Chulmin'\").show() # select * from data_sdf where Name = 'Chulmin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b951bccb-b3ab-4067-b7d2-278a382f1e46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_sdf.filter( (data_sdf['Gender'] == 'Male') & (col('Year') > 2011) ).show() # select * from data_sdf where Gender = 'Male' and Year > 2011\n",
    "data_sdf.filter( (data_sdf['Gender'] == 'Male') | (col('Year') < 2011) ).show() # select * from data_sdf where Gender = 'Male' or Year < 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81b3bcb8-17d9-4777-a410-c152d29f2c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 문자열 컬럼의 like 조건 수행. \n",
    "print('문자열 컬럼 like() filter() 수행. ')\n",
    "data_sdf.filter(col('Name').like('Chul%')).show() # select * from data_sdf where Name like 'Chul%'\n",
    "\n",
    "print('SQL의 Like 조건문을 string으로 filter() 수행. ')\n",
    "data_sdf.filter(\" Name like 'Chul%' \").show()\n",
    "data_sdf.filter(\" Name like 'C%' \").show() # startswith('C') where Name like 'C%'\n",
    "data_sdf.filter(\" Name like '%u%' \").show() # contains('u')\n",
    "data_sdf.filter(\" upper(Name) like '%M%' \").show() #별도의 pyspark.sql.functions 호출없이 수행 가능.  select * from data_sdf where upper(Name) like '%M%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8df0c99e-f22b-459d-9c5d-85a0256fe0f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "data_sdf.filter(upper(data_sdf['Name']).like('%M%')).show() #select * from data_sdf where upper(Name) like '%M%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0571ab76-13ec-487a-a1c9-0b2af8761ee2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filtering 후에 특정 컬럼으로만 DataFrame 생성하려면 select() 적용. \n",
    "data_sdf.filter(upper(data_sdf['Name']).like('%M%')).select('Year', 'Gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55685168-98b9-450d-ab9c-67b638f8452b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_dataframe_exercise_01",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
